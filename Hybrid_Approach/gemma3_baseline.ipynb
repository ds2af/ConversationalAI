{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca276b9c-07e9-443d-9f0c-ddfe140b600d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2758fe-a71d-4c8f-a116-5886b3759846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1523 tweets from ../data/test.csv\n",
      "\n",
      "=== Classifying tweets using Ollama model: gemma3:4b ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:  51%|████████████████████████████████▉                               | 784/1523 [03:28<03:07,  3.94tweet/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Could not parse response or error occurred for tweet: GUN FIREARM WEAPON VECTOR CLIP ART FOR VINYL SIGN ... | Response: '0; This tweet does not relate to any disaster or emergency situation; it's an image related to firearms.' | Error: Expected 2 parts separated by ';' but got 3. Response: '0; This tweet does not relate to any disaster or emergency situation; it's an image related to firearms.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying: 100%|███████████████████████████████████████████████████████████████| 1523/1523 [06:42<00:00,  3.78tweet/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed results saved to classification_results/classification_evaluation_details.csv and classification_results/classification_evaluation_details.json\n",
      "\n",
      "=== Classification Metrics ===\n",
      "\n",
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0          600          273\n",
      "Actual 1          115          534\n",
      "\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "Not Disaster-Related (0)     0.8392    0.6873    0.7557       873\n",
      "    Disaster-Related (1)     0.6617    0.8228    0.7335       649\n",
      "\n",
      "                accuracy                         0.7451      1522\n",
      "               macro avg     0.7504    0.7550    0.7446      1522\n",
      "            weighted avg     0.7635    0.7451    0.7462      1522\n",
      "\n",
      "\n",
      "Evaluation complete. Check 'classification_results/' for detailed results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "# Import scikit-learn metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    accuracy_score # Can use this explicitly too\n",
    ")\n",
    "# matplotlib is no longer needed as ROC curve is removed\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure the Ollama server is running with the specified model available.\n",
    "# Example: ollama run gemma:2b\n",
    "\n",
    "# --- Configuration ---\n",
    "OLLAMA_MODEL = 'gemma3:4b' # <<< ADJUST your Ollama model name here if needed\n",
    "INPUT_CSV = \"../data/test.csv\" # <<< Path to your test CSV (must have 'text' and 'labels' columns)\n",
    "OUTPUT_DIR = \"classification_results\"\n",
    "RESULTS_FILENAME = \"classification_evaluation\"\n",
    "\n",
    "# -- Context prompt for classification (No confidence score) --\n",
    "context_prompt = (\n",
    "    '''\n",
    "    I am a researcher interested in studying disaster response events. My goal is to analyze historical tweets to better understand responses to various types of disasters.\n",
    "\n",
    "Each tweet I receive includes a disaster-related keyword or hashtag, but not all tweets describe an actual disaster event. I need a model to classify tweets into two categories based on whether they are truly disaster-related:\n",
    "1 - Disaster-Related (tweets that report actual emergencies, accidents, crises, or factual news about such events requiring attention).\n",
    "0 - Not Disaster-Related (tweets that use disaster-related terms metaphorically, humorously, or in other non-emergency contexts).\n",
    "\n",
    "Consider these guidelines for classification:\n",
    "- Tweets reporting actual emergencies, accidents, or crises (such as natural disasters, transportation accidents, industrial incidents, or health emergencies) should be classified as 1.\n",
    "- Factual news reports about any actual disaster or accident should be classified as 1.\n",
    "- Tweets using disaster-related terminology metaphorically, humorously, or in a non-urgent manner should be classified as 0.\n",
    "- Jokes, memes, or casual mentions that do not indicate real-life emergencies should be classified as 0.\n",
    "\n",
    "Please return the classification (0 or 1) followed by a brief reasoning for your decision. These two parts—classification and reasoning—should be returned in one line, separated by a single semicolon (;).\n",
    "\n",
    "For example:\n",
    "1; This tweet mentions rescue efforts and evacuation details following a reported flood.\n",
    "\n",
    "Please return your answer in this exact format:\n",
    "classification; reasoning\n",
    "where classification is either 0 or 1, and reasoning is a short explanation.\n",
    "\n",
    "Important: When you write the reasoning, do not use any semicolons (;) within the reasoning text itself, as your answer will be processed by a script expecting only one semicolon separator in the entire line.\n",
    "\n",
    "'''\n",
    ")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def fix_tweet_text(tweet: str) -> str:\n",
    "    \"\"\"Clean up tweets by removing extra whitespace.\"\"\"\n",
    "    # Ensure tweet is a string before processing\n",
    "    if not isinstance(tweet, str):\n",
    "        tweet = str(tweet)\n",
    "    return re.sub(r'\\s+', ' ', tweet).strip()\n",
    "\n",
    "def classify_tweet(tweet: str, model_name: str) -> str:\n",
    "    \"\"\"Call the Ollama model and return the raw response string.\"\"\"\n",
    "    full_prompt = f\"{context_prompt}\\nTweet:\\n{tweet}\"\n",
    "    try:\n",
    "        resp = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': full_prompt}]\n",
    "            # Consider adding options like temperature if needed:\n",
    "            # options={'temperature': 0.0} # For more deterministic output\n",
    "        )\n",
    "        # Ensure the response content exists and is a string\n",
    "        message_content = resp.get('message', {}).get('content', '')\n",
    "        if not isinstance(message_content, str):\n",
    "             return f\"error; Invalid response format from Ollama: {type(message_content)}\"\n",
    "        return message_content\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError calling Ollama model '{model_name}': {e}\")\n",
    "        # Return an error string that somewhat matches expected format but indicates failure\n",
    "        return f\"error; Ollama API call failed: {e}\"\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    # 1. Load your data\n",
    "    try:\n",
    "        test_df = pd.read_csv(INPUT_CSV)\n",
    "        # Ensure required columns exist\n",
    "        if 'text' not in test_df.columns or 'labels' not in test_df.columns:\n",
    "            raise ValueError(\"CSV must contain 'text' and 'labels' columns.\")\n",
    "        print(f\"Loaded {len(test_df)} tweets from {INPUT_CSV}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at {INPUT_CSV}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Prepare output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # 3. Process tweets and collect results\n",
    "    results_data = [] # Store detailed results per tweet\n",
    "    all_actual_labels = []\n",
    "    all_predicted_labels = []\n",
    "    # Confidence scores are no longer collected from the model\n",
    "\n",
    "    print(f\"\\n=== Classifying tweets using Ollama model: {OLLAMA_MODEL} ===\")\n",
    "    # Progress bar over tweets\n",
    "    for _, row in tqdm(\n",
    "            test_df.iterrows(),\n",
    "            total=len(test_df),\n",
    "            desc=\"Classifying\",\n",
    "            unit=\"tweet\"\n",
    "    ):\n",
    "        tweet = row['text']\n",
    "        # Ensure actual label is integer 0 or 1\n",
    "        try:\n",
    "            actual_label = int(row['labels'])\n",
    "            if actual_label not in [0, 1]:\n",
    "                 raise ValueError(\"Actual label must be 0 or 1\")\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"\\nSkipping row due to invalid actual label: {row['labels']} ({e}). Tweet: {str(tweet)[:50]}...\")\n",
    "            results_data.append({\n",
    "                \"tweet\": fix_tweet_text(tweet),\n",
    "                \"predicted_label\": \"error\",\n",
    "                # \"confidence\": None, # Confidence no longer provided\n",
    "                \"reasoning\": \"\",\n",
    "                \"actual_label\": row['labels'], # Keep original problematic label\n",
    "                \"raw_response\": \"\",\n",
    "                \"error\": f\"Invalid actual label: {e}\"\n",
    "            })\n",
    "            continue # Skip this row\n",
    "\n",
    "        tweet_clean = fix_tweet_text(tweet)\n",
    "        raw_resp = classify_tweet(tweet_clean, OLLAMA_MODEL)\n",
    "\n",
    "        # Parse the response (expecting \"classification; reasoning\")\n",
    "        pred_label_parsed = \"error\"\n",
    "        # confidence_parsed = None # Confidence no longer provided\n",
    "        reasoning_parsed = \"\"\n",
    "        error_msg = None\n",
    "\n",
    "        try:\n",
    "            # Handle potential None or non-string responses explicitly\n",
    "            if not isinstance(raw_resp, str):\n",
    "                 raise TypeError(f\"Received non-string response from classify_tweet: {type(raw_resp)}\")\n",
    "\n",
    "            parts = raw_resp.split(\";\")\n",
    "            # Be more robust: handle potential extra/missing whitespace\n",
    "            parts = [p.strip() for p in parts]\n",
    "\n",
    "            # --- PARSING LOGIC UPDATED ---\n",
    "            if len(parts) != 2: # Expect exactly 2 parts now\n",
    "                 # Check if it might be an error message returned from classify_tweet\n",
    "                 if parts[0].lower() == 'error':\n",
    "                      raise ValueError(f\"Ollama API or internal error: {parts[1] if len(parts)>1 else raw_resp}\")\n",
    "                 else:\n",
    "                      raise ValueError(f\"Expected 2 parts separated by ';' but got {len(parts)}. Response: '{raw_resp}'\")\n",
    "\n",
    "            pred_label_parsed = int(parts[0])\n",
    "            if pred_label_parsed not in [0, 1]:\n",
    "                raise ValueError(f\"Predicted label must be 0 or 1, but got {pred_label_parsed}\")\n",
    "\n",
    "            reasoning_parsed = parts[1] # Reasoning is the second part\n",
    "            # --- END PARSING LOGIC UPDATE ---\n",
    "\n",
    "            # If parsing successful, add to lists for sklearn metrics\n",
    "            all_actual_labels.append(actual_label)\n",
    "            all_predicted_labels.append(pred_label_parsed)\n",
    "            # No confidence score to append\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Parsing or processing error: {e}\"\n",
    "            # Do not add to sklearn lists if parsing failed\n",
    "            print(f\"\\nWarning: Could not parse response or error occurred for tweet: {tweet_clean[:50]}... | Response: '{raw_resp}' | Error: {e}\")\n",
    "\n",
    "        # Append detailed result regardless of parsing success\n",
    "        results_data.append({\n",
    "            \"tweet\": tweet_clean,\n",
    "            \"predicted_label\": pred_label_parsed,\n",
    "            # \"confidence\": confidence_parsed, # No longer applicable\n",
    "            \"reasoning\": reasoning_parsed,\n",
    "            \"actual_label\": actual_label,\n",
    "            \"raw_response\": raw_resp,\n",
    "            \"error\": error_msg # None if parsing succeeded and no other error\n",
    "        })\n",
    "\n",
    "    # 4. Save detailed results\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    # Add confidence column back with None values if needed for schema consistency downstream\n",
    "    # results_df['confidence'] = None # Optional, depends if you need the column\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"{RESULTS_FILENAME}_details.csv\")\n",
    "    json_path = os.path.join(OUTPUT_DIR, f\"{RESULTS_FILENAME}_details.json\")\n",
    "    try:\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        results_df.to_json(json_path, orient='records', indent=2)\n",
    "        print(f\"\\nDetailed results saved to {csv_path} and {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving detailed results: {e}\")\n",
    "\n",
    "\n",
    "    # 5. Calculate and Print Metrics (only if there are valid predictions)\n",
    "    print(\"\\n=== Classification Metrics ===\")\n",
    "    if not all_actual_labels:\n",
    "        print(\"No valid predictions were successfully parsed. Cannot calculate metrics.\")\n",
    "        return\n",
    "\n",
    "    # Ensure labels are integers for sklearn functions\n",
    "    try:\n",
    "        all_actual_labels_int = [int(l) for l in all_actual_labels]\n",
    "        all_predicted_labels_int = [int(l) for l in all_predicted_labels]\n",
    "    except ValueError:\n",
    "        print(\"Error: Could not convert all labels to integers for metric calculation.\")\n",
    "        return\n",
    "\n",
    "    # -- Confusion Matrix --\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    try:\n",
    "        cm = confusion_matrix(all_actual_labels_int, all_predicted_labels_int)\n",
    "        # Handle case where CM might not be 2x2 if only one class was predicted/present\n",
    "        if cm.shape == (1, 1):\n",
    "             print(f\"Only one class present/predicted: {cm}\")\n",
    "             # Manually create a 2x2 matrix for consistent display if needed\n",
    "             if all_actual_labels_int[0] == 0: # Only class 0\n",
    "                 cm_display = pd.DataFrame([[cm[0,0], 0], [0, 0]], index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "             else: # Only class 1\n",
    "                 cm_display = pd.DataFrame([[0, 0], [0, cm[0,0]]], index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "             print(cm_display)\n",
    "        elif cm.shape == (2,2):\n",
    "             print(pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1']))\n",
    "        else:\n",
    "             print(f\"Unexpected confusion matrix shape: {cm.shape}\")\n",
    "             print(cm)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating/displaying Confusion Matrix: {e}\")\n",
    "\n",
    "\n",
    "    # -- Classification Report --\n",
    "    print(\"\\nClassification Report:\")\n",
    "    try:\n",
    "        report = classification_report(\n",
    "            all_actual_labels_int,\n",
    "            all_predicted_labels_int,\n",
    "            target_names=['Not Disaster-Related (0)', 'Disaster-Related (1)'],\n",
    "            digits=4, # Show more precision\n",
    "            zero_division=0 # Set how to handle zero division (e.g., for precision/recall when no predictions/actuals for a class)\n",
    "        )\n",
    "        print(report)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Classification Report: {e}\")\n",
    "\n",
    "    # -- ROC AUC Score and Curve are REMOVED as confidence score is not available --\n",
    "\n",
    "    print(f\"\\nEvaluation complete. Check '{OUTPUT_DIR}/' for detailed results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8aced1-b2ff-4e20-a765-99ef0ebfae49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
